{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6c33c7b-bcb3-42c0-b55a-eca0bb4530f3",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src = \"../assets/dask-logo.svg\" alt=\"Dask logo\" width=\"20%\">\n",
    "</div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087fc997-f3c4-4394-b17a-768d69954bba",
   "metadata": {},
   "source": [
    "# Example: Using Dask, Dask Gateway, and adaptive scaling\n",
    "\n",
    "In this notebook, we will explore some [Indian stock market data](https://www.kaggle.com/datasets/debashis74017/stock-market-data-nifty-50-stocks-1-min-data?select=ASIANPAINT_minute_data_with_indicators.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a469b0a2-b787-4040-b0f4-33a328a2d1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location of Indian stock market data on Google Storage\n",
    "data_uri = \"gs://nebari-public/nifty_stock_market_data/stock-market-data-india\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5707a5f-b51e-4c2c-beee-db3d665262d4",
   "metadata": {},
   "source": [
    "## Explore a dataset with Dask DataFrame\n",
    "\n",
    "But first let's see how large this dataset is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ff5aea-b04b-43ed-908f-7dc2ce5fc42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the size of the dataset using gscfs\n",
    "from gcsfs import GCSFileSystem  \n",
    "\n",
    "gs = GCSFileSystem()\n",
    "print(f\"{gs.du(data_uri) / 1e9} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6648062-a326-4862-90cb-8bd93ba2ffc2",
   "metadata": {},
   "source": [
    "In the following notebook cells you will load the stocks dataset into a Dask DataFrame and view the first few elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba54e2c4-cca5-4c16-8807-671c1d01cc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dask's Dask DataFrame API\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ac271c-4a14-4f5c-be1d-f7391c8c3004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV files using a glob-pattern into a Dask DataFrame\n",
    "ddf = dd.read_csv(data_uri + \"/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053a1a07-e717-4faf-8fea-b1eb26a7342c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the lazy Dask DataFrame\n",
    "ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daecef38-6acc-4119-b473-e5151a7800eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the first few values of the DataFrame (`head` calls `compute` internally)\n",
    "ddf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b723a83d-6489-43fc-88d3-46947b664cef",
   "metadata": {},
   "source": [
    "**Convert your Dask DataFrame to a pandas DataFrame to load the entire dataset into local memory.**\n",
    "\n",
    "> ‚ö†Ô∏è Warning! This will crash your kernel because of insufficient local memory! You'll need to restart the kernel and read the dataset in again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94d81c8-08f3-4be9-ac4d-3c94fd946f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a Dask DataFrame to pandas DataFrame\n",
    "# Uncomment the next line to run. This will crash your kernel!\n",
    "\n",
    "# df = ddf.high.max().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a30182-4cea-4084-a937-dd88eda03463",
   "metadata": {},
   "source": [
    "As we mentioned earlier, Dask computations look very similar to pandas with an extra `compute()` at the end.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fd8c6c-cd07-419f-8ad1-7fadefd14120",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Scale to large dataset with Dask Gateway\n",
    "\n",
    "You can now scale your computation to all the ~100 files in the dataset using a Dask cluster with Dask Gateway."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcf94b0-b6d3-412e-b9df-10cb16d2ad0b",
   "metadata": {},
   "source": [
    "### Create a Dask Gateway instance\n",
    "\n",
    "As the first step, import and instantiate Dask Gateway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9254e87-d550-4726-a8c0-c22a77973596",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dask_gateway import Gateway\n",
    "\n",
    "gateway = Gateway()\n",
    "gateway"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911f4151-026c-4a95-b4e8-7b8243b018b9",
   "metadata": {},
   "source": [
    "Open the `Cluster Options` widget where you can view and update cluster configurations like the conda environment, instance type, and any environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5eb6ff-a627-4342-93e4-9613ccd1cec4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "options = gateway.cluster_options()\n",
    "options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f12191-5f76-4ab4-9610-611fe57d8d36",
   "metadata": {},
   "source": [
    "This is a visual example, but all of this can of course be done programatically:\n",
    "\n",
    "```python\n",
    "options.conda_environment = conda_env\n",
    "options.profile = worker_type\n",
    "options.environment_vars = {\"MYENV\": \"aNeNvVaR\"}\n",
    "```\n",
    "\n",
    "> ‚ö†Ô∏è Warning: It's important that the environment used for your notebook (that is, the IPython kernel) must match the Dask worker environment (that is, `options.conda_environment`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78ba37e-b0cd-4a44-8d29-fba5f8dcbd93",
   "metadata": {},
   "source": [
    "### Create a new Dask cluster and connect to a Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296032fb-471d-4eb0-b8ac-07a7053323d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new cluster with the above options\n",
    "cluster = gateway.new_cluster(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2053b088-46f5-41d3-bbaa-ab4df8e45cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the cluster widget\n",
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe477a1c-7289-41cd-9fab-019b3caaa1c7",
   "metadata": {},
   "source": [
    "> Once the cluster is initialized, you'll need to log in to the dashboard via Keycloak before connecting to it using the JupyterLab extension. Click on the dashboard link above and log in now.\n",
    "\n",
    "\n",
    "The cluster starts with zero workers, so you need to set number of workers manually or setup **adaptive scaling**. With adaptive, your cluster can automatically resize itself within the minimum and maximum bounds based on the workload. Learn more in Dask's [adaptive deployments documentation](https://docs.dask.org/en/stable/how-to/adaptive.html).\n",
    "\n",
    "**In the above UI, set up adaptive with 1 minimum node and 10 maximum nodes.**\n",
    "\n",
    "Image source: [Dask documentation](https://docs.dask.org/en/stable/how-to/adaptive.html)\n",
    "\n",
    "<img src=\"../assets/dask-adaptive.svg\" alt=\"Dask adaptive scaling\" width=\"30%\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481a79de-3718-42eb-9a47-b4f15b2586f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable adaptive scaling\n",
    "cluster.adapt(minimum=1, maximum=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fdec9b-6df2-4196-bae6-ccf171205c37",
   "metadata": {},
   "source": [
    "To use adaptive scaling programmatically:\n",
    " \n",
    "```python\n",
    "cluster.adapt(minimum=1, maximum=10)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa7042e-bbbc-4c6b-bbd8-988cb7b6f451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect a new client to the Gateway cluster\n",
    "client = cluster.get_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c4d5a3-d880-4895-bd3d-8c4fb1305d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the client widget\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4449f494-dd2e-43fe-a48b-4cbc80067497",
   "metadata": {},
   "source": [
    "The `Dask Client` interface gives us a brief summary of everything we've set up so far. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e1017c-7858-4fbe-b2d2-b9689f032d6f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Dask's diagnostic dashboard\n",
    "\n",
    "Open the Dask dashboard by clicking on the link in the Client UI.\n",
    "\n",
    "Or (recommended), using the JupyterLab extension in the left sidebar, open:\n",
    "\n",
    "* Cluster map\n",
    "* Task stream\n",
    "* Progress bar\n",
    "* Worker memory plots (Optional)\n",
    "* Task groups plot (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70de4c41-6a73-4c88-98f2-770e88f07b8a",
   "metadata": {},
   "source": [
    "## Computation on the large dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67197670-0ca6-4a40-a8b1-8d57015b68ad",
   "metadata": {},
   "source": [
    "### Stock data compute\n",
    "\n",
    "With the Dask cluster running, we have the resources to do some computation!\n",
    "\n",
    "Let's compute the highest `high` and lowest `low`. Make sure to look at the dashboard plots!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f0d641-92be-44f2-8508-c96b7159ece2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute highest-high\n",
    "# Uncomment the next line to run. This will NOT crash your kernel, \n",
    "# but it might take a little while as workers get spun up.\n",
    "\n",
    "# ddf.high.max().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9977717c-b86a-471a-a53d-6e8d54f4adcc",
   "metadata": {},
   "source": [
    "### Standalone example with Dask Array\n",
    "\n",
    "The previous example reads data from cloud storage, which can take time. Here is an example with Dask Array that you can execute immediately!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84295fb2-1dfd-477e-9b12-3ca474dc6c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0900c32-dc7b-435b-b8d9-5e9978b40569",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = da.random.random((100000, 100000), chunks=(1000, 1000))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f26369-e7b5-4255-b50f-c510c0b732d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x * x\n",
    "z = y.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5491414a-ae69-42ab-904e-7d8c7fb96ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6ddb95-493b-4b31-b41f-ca1f7cb47d3e",
   "metadata": {},
   "source": [
    "## Shutdown the cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72775a8b-45c5-47da-a861-4c1efc31154d",
   "metadata": {},
   "source": [
    "**ALWAYS** remember to shutdown your cluster with the following commands.\n",
    "\n",
    "> ‚ö†Ô∏è Warning: As with JupyterLab servers, Dask workers run on cloud compute instances and cost actual money."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09bbbaf-9d0b-415f-8ea4-3e38caffd47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3708645-d4e8-43f0-80db-aec1fddba2b3",
   "metadata": {},
   "source": [
    "---\n",
    "## üëè Next:\n",
    "* [04_visualizations_and_dashboards](../04_visualizations_and_dashboards.ipynb)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caebc8b1-bb7e-40c7-b533-83482b496735",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global-global-nebari-demo",
   "language": "python",
   "name": "conda-env-global-global-nebari-demo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
